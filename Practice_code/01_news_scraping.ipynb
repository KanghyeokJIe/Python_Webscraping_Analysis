{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d721a30a",
   "metadata": {},
   "source": [
    "# 📘 1-1) Daum 뉴스 기사 제목 스크래핑하기\n",
    "\n",
    "## ✅ 문제 설명\n",
    "\n",
    "- 다음 URL에서 뉴스 기사들의 **링크와 제목을 스크래핑**하세요.\n",
    "- 뉴스 기사는 총 9개가 존재하며, 모두 출력되어야 합니다.\n",
    "- 요청 결과는 반드시 `utf-8`로 인코딩하여 처리해야 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 대상 URL\n",
    "\n",
    "https://news.daum.net/economy\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 실행 결과 예시\n",
    "\n",
    "- https://v.daum.net/v/20250408180205986  \n",
    "  **[단독] \"돈 잘 버는 알짜부터\"…SK '몸값 5조' 실트론 매각한다**\n",
    "\n",
    "- https://v.daum.net/v/20250408175343664  \n",
    "  **美·中 관세 폭탄 돌리기…‘새우등’ 한국 경제 불안감 더 커졌다**\n",
    "\n",
    "- https://v.daum.net/v/20250408175111585  \n",
    "  **나랏빚 1175조 '역대 최대'…추경 재원 바닥**\n",
    "\n",
    "- https://v.daum.net/v/20250408174344325  \n",
    "  **메디젠휴먼케어, \"23andMe 파산..한국 유전체 산업의 위기 아닌 기회\"**\n",
    "\n",
    "- https://v.daum.net/v/20250408173713085  \n",
    "  **칩 확보서 데이터센터까지···전 산업에 'AI 고속도로' 깔아야**\n",
    "\n",
    "- https://v.daum.net/v/20250408173626057  \n",
    "  **77년 묵은 정부조직, 데이터 중심 대수술···'AI 부총리' 도입을**\n",
    "\n",
    "- https://v.daum.net/v/20250408172845794  \n",
    "  **[이슈ON] 마은혁 받고 '윤 절친' 이완규 투입...한덕수는 왜?**\n",
    "\n",
    "- https://v.daum.net/v/20250408161553109  \n",
    "  **트럼프발 주가폭락에 동아 \"경제적 핵전쟁\" 조선 \"금융위기 후 최악\"**\n",
    "\n",
    "- https://v.daum.net/v/20250408160526681  \n",
    "  **공황과 격변의 판도라 박스가 열리다**\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 요구 사항\n",
    "\n",
    "- `requests`로 웹페이지 요청\n",
    "- `res.encoding = 'utf-8'` 설정 (한글 깨짐 방지)\n",
    "- `BeautifulSoup`을 사용해 HTML 파싱\n",
    "- `ul.list_newsheadline2` 내부의 뉴스 링크와 제목 추출\n",
    "- 링크는 `<a href=\"...\">`에서, 제목은 `<strong class=\"tit_txt\">`에서 추출\n",
    "- 총 9개의 뉴스 제목과 링크를 순서대로 출력\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 학습 포인트\n",
    "\n",
    "- 웹 스크래핑 시 **적절한 인코딩 설정** (`res.encoding`)\n",
    "- CSS 선택자를 이용한 **BeautifulSoup의 select() 사용법**\n",
    "- `.text`, `['href']` 속성 사용을 통한 **텍스트 및 링크 추출**\n",
    "- 출력 순서와 포맷을 **문제 조건에 맞게 정리하여 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad797815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.daum.net/economy\n",
      "<class 'requests.models.Response'>\n",
      "200\n",
      "<class 'bs4.element.ResultSet'> 9\n",
      "https://v.daum.net/v/20250408180205986\n",
      "[단독] \"돈 잘 버는 알짜부터\"…SK '몸값 5조' 실트론 매각한다\n",
      "https://v.daum.net/v/20250408175343664\n",
      "美·中 관세 폭탄 돌리기…‘새우등’ 한국 경제 불안감 더 커졌다\n",
      "https://v.daum.net/v/20250408175111585\n",
      "나랏빚 1175조 '역대 최대'…추경 재원 바닥\n",
      "https://v.daum.net/v/20250408174344325\n",
      "메디젠휴먼케어, \"23andMe 파산..한국 유전체 산업의 위기 아닌 기회\"\n",
      "https://v.daum.net/v/20250408173713085\n",
      "칩 확보서 데이터센터까지···전 산업에 'AI 고속도로' 깔아야\n",
      "https://v.daum.net/v/20250408173626057\n",
      "77년 묵은 정부조직, 데이터 중심 대수술···'AI 부총리' 도입을\n",
      "https://v.daum.net/v/20250408172845794\n",
      "[이슈ON] 마은혁 받고 '윤 절친' 이완규 투입...한덕수는 왜?\n",
      "https://v.daum.net/v/20250408161553109\n",
      "트럼프발 주가폭락에 동아 \"경제적 핵전쟁\" 조선 \"금융위기 후 최악\"\n",
      "https://v.daum.net/v/20250408160526681\n",
      "공황과 격변의 판도라 박스가 열리다\n"
     ]
    }
   ],
   "source": [
    "import requests     \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 요청 헤더 설정 (브라우저처럼 보이게 하기 위한 User-Agent 설정)\n",
    "req_header = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# 2. URL에 넣을 파라미터 (sid: 섹션 아이디 - economy = 경제 섹션)\n",
    "req_param = {\n",
    "    'sid': 'economy'\n",
    "}\n",
    "\n",
    "# 3. 실제 요청할 URL 생성\n",
    "url = 'https://news.daum.net/{sid}'.format(**req_param)\n",
    "print(url)  # → https://news.daum.net/economy\n",
    "\n",
    "# 4. 웹 서버에 요청 (헤더 포함)\n",
    "res = requests.get(url, headers=req_header)\n",
    "\n",
    "# 5. 인코딩 설정 (중요! 한글 깨짐 방지)\n",
    "res.encoding = 'utf-8'\n",
    "\n",
    "# 6. 응답 상태 및 타입 확인\n",
    "print(type(res))           # <class 'requests.models.Response'>\n",
    "print(res.status_code)     # 200이면 정상\n",
    "\n",
    "# 7. 응답이 정상일 때만 파싱 진행\n",
    "if res.ok:\n",
    "    html = res.text  # HTML 코드 문자열\n",
    "    soup = BeautifulSoup(html, 'html.parser')  # BeautifulSoup로 HTML 파싱\n",
    "\n",
    "    # 8. 뉴스 기사 링크가 포함된 a 태그 리스트 추출 (href에 '/v.daum.net' 포함)\n",
    "    li_tag_list = soup.select(\"ul.list_newsheadline2 a[href*='/v.daum.net']\")\n",
    "    print(type(li_tag_list), len(li_tag_list))  # 몇 개 있는지 출력\n",
    "\n",
    "    # 9. 각각의 뉴스 항목(li 태그) 순회하며 제목과 링크 출력\n",
    "    for li_tag in soup.select('ul.list_newsheadline2 li'):\n",
    "        a_tag = li_tag.find('a')  # 뉴스 기사로 가는 링크\n",
    "        link = a_tag['href']      # href 속성값 (URL)\n",
    "        print(link)\n",
    "\n",
    "        # 뉴스 제목이 담긴 strong 태그 선택\n",
    "        strong_tag = li_tag.select_one('div.cont_thumb strong.tit_txt')\n",
    "        title = strong_tag.text.strip()  # 텍스트 추출 후 공백 제거\n",
    "        print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d135716",
   "metadata": {},
   "source": [
    "# 📘 1-2) 섹션 선택 함수로 뉴스 기사 스크래핑하기\n",
    "\n",
    "## ✅ 문제 설명\n",
    "\n",
    "- 여러 뉴스 섹션 중 하나를 선택하여, 해당 뉴스 URL에서 **기사의 링크와 제목을 스크래핑**하는 함수를 작성하세요.\n",
    "- 사용자가 한글로 섹션명을 입력하면, 사전에 정의된 딕셔너리를 통해 **영문 코드로 변환하여 URL을 구성**해야 합니다.\n",
    "- 이후 동작은 1-1 문제와 동일하게 처리하면 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 대상 URL\n",
    "\n",
    "아래는 섹션별 예시 URL입니다. 사용자가 선택한 섹션에 따라 아래 중 하나가 사용됩니다.\n",
    "\n",
    "- 경제: `https://news.daum.net/economy`  \n",
    "- 사회: `https://news.daum.net/society`  \n",
    "- 인물: `https://news.daum.net/people`  \n",
    "*(기타 섹션도 `https://news.daum.net/{영문코드}` 형식)*\n",
    "\n",
    "---\n",
    "\n",
    "## 🧾 실행 결과 예시 (리스트 형식)\n",
    "\n",
    "- `print_news('경제')`  \n",
    "  ======> https://news.daum.net/economy 경제 뉴스 <======\n",
    "\n",
    "  - https://v.daum.net/v/20250408181016275  \n",
    "    **관세 다음은 환율전쟁? 美 ‘100년물 미국채 강매’까지 만지작**\n",
    "\n",
    "  - https://v.daum.net/v/20250408180205986  \n",
    "    **[단독] \"돈 잘 버는 알짜부터\"…SK '몸값 5조' 실트론 매각한다**\n",
    "\n",
    "  - https://v.daum.net/v/20250408175343664  \n",
    "    **美·中 관세 폭탄 돌리기…‘새우등’ 한국 경제 불안감 더 커졌다**\n",
    "\n",
    "  ...\n",
    "\n",
    "- `print_news('사회')`  \n",
    "  ======> https://news.daum.net/society 사회 뉴스 <======\n",
    "\n",
    "  - https://v.daum.net/v/20250408181101293  \n",
    "    **계엄 문건, ‘세월호 7시간’처럼 봉인되나…열쇠는 피의자 한덕수 대행 손에**\n",
    "\n",
    "  - https://v.daum.net/v/20250408181007271  \n",
    "    **법조계 “한덕수의 재판관 지명 월권”…헌법소원·효력정지 가처분 거론**\n",
    "\n",
    "  ...\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 요구 사항\n",
    "\n",
    "- `section_dict`를 활용하여 한글 섹션명을 영문 코드로 변환\n",
    "- 변환된 코드를 사용해 URL 생성 → 요청 → HTML 파싱\n",
    "- 뉴스 기사는 `<ul class=\"list_newsheadline2\">` 내에서 `<a>` 링크 및 `<strong class=\"tit_txt\">` 제목 추출\n",
    "- `print_news(섹션명)` 함수로 호출 가능해야 함\n",
    "- `res.encoding = 'utf-8'` 인코딩 설정 필수\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 학습 포인트\n",
    "\n",
    "- `딕셔너리 매핑`을 활용한 입력 처리 및 유연한 URL 구성\n",
    "- `함수화`를 통한 코드 재사용성과 확장성 확보\n",
    "- `BeautifulSoup`을 사용한 `select()`, `select_one()`로 원하는 태그 추출\n",
    "- 웹 페이지 크롤링 시 **인코딩 설정** 중요성 이해\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6b95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> https://news.daum.net/economy 경제 뉴스 <=====\n",
      "https://v.daum.net/v/20250409125903840\n",
      "천안시 '인구 변화 대응', 출생·양육~미래 산업 육성…인구 유입 정책 추진\n",
      "https://v.daum.net/v/20250409122036180\n",
      "정부, 美에 LNG 수입·조선업 협력·알래스카 투자 제안…트럼프  “Great Call”\n",
      "https://v.daum.net/v/20250409120018573\n",
      "그래도 꾸준한 프랜차이즈…재작년 매출액 소상공인 1.75배\n",
      "https://v.daum.net/v/20250409120009533\n",
      "3월 은행 가계대출 1.4조↑ \"토허제發 주택거래 급증은 2분기 영향\"\n",
      "https://v.daum.net/v/20250409114312833\n",
      "高관세·高환율 쓰나미…희비 엇갈린 산업계\n",
      "https://v.daum.net/v/20250409113001204\n",
      "美·日은 '죽은 원전'도 되살린 판에…한국은 2년간 뭐했나 [김리안의 에네르기파WAR]\n",
      "https://v.daum.net/v/20250409112842145\n",
      "취업자 석달째 늘었지만 청년·건설 '고용한파'…관세조치에 더 암울(종합2보)\n",
      "https://v.daum.net/v/20250409111613474\n",
      "\"아이폰 곧 300만원\" 트럼프, 중국 잡으려다 시총 1위 애플 잡네\n",
      "https://v.daum.net/v/20250409111311272\n",
      "美조선 수주, 韓이 싹쓸이…K조선의 ‘지정학 반사이익’ 현실 [한양경제]\n",
      "=====> https://news.daum.net/society 사회 뉴스 <=====\n",
      "https://v.daum.net/v/20250409123807559\n",
      "민주주의 취약하면 파시즘이 온다[새 책]\n",
      "https://v.daum.net/v/20250409121612104\n",
      "'포트트릭' 몰아친 日 축구 국가대표, 가수 김정민 아들이었다\n",
      "https://v.daum.net/v/20250409120605878\n",
      "\"숲가꾸기-임도 현장일수록 산불 피해 극심\"\n",
      "https://v.daum.net/v/20250409115100177\n",
      "“과일로 간신히 연명합니다”…원화값 폭락에 유학생들 ‘멘붕’\n",
      "https://v.daum.net/v/20250409114514933\n",
      "“폭력 등 한국 극단주의 행동, 조기대선에 더 심화될 우려”\n",
      "https://v.daum.net/v/20250409113111280\n",
      "\"65세가 노인? 요즘은 70∼74세가 진짜 노인…정년 연장해야\"\n",
      "https://v.daum.net/v/20250409112842145\n",
      "취업자 석달째 늘었지만 청년·건설 '고용한파'…관세조치에 더 암울(종합2보)\n",
      "https://v.daum.net/v/20250409112528988\n",
      "하루 10분 아이와 깊은 대화…학교생활, 학습태도가 좋아집니다\n",
      "https://v.daum.net/v/20250409111105152\n",
      "전남도,친환경 자동차 기반 넷제로 시티 실증사업 온힘\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 한글 → 영어 섹션 코드 딕셔너리\n",
    "# 뉴스 URL에는 영어 코드가 들어가고, 출력에는 한글을 그대로 쓰기 위해 사용\n",
    "section_dict = {\n",
    "    '최신뉴스': 'climate',\n",
    "    '사회': 'society',\n",
    "    '경제': 'economy',\n",
    "    '정치': 'politics',\n",
    "    '국제': 'world',\n",
    "    '문화': 'culture',\n",
    "    '생활': 'life',\n",
    "    'IT/과학': 'tech',\n",
    "    '인물': 'people'\n",
    "}\n",
    "\n",
    "# 뉴스 크롤링 함수 정의 [해당 함수의 정의에서 한글로 입력을 받으면 economy뉴스 와 같이 출력되어 해당 값을 GPT에게 수정 요청]\n",
    "def print_news(section_name):\n",
    "    # ★ GPT 수정된 부분 1: 한글 → 영어 코드 변환\n",
    "    section_code = section_dict[section_name]  # 예: '경제' → 'economy'\n",
    "    # ★ GPT 수정된 부분 2: URL에 영어 코드 삽입\n",
    "    url = f\"https://news.daum.net/{section_code}\"  # 예: https://news.daum.net/economy\n",
    "    # ★ GPT 수정된 부분 3: 출력에는 한글 그대로 사용\n",
    "    print(f\"=====> {url} {section_name} 뉴스 <=====\")  # 예: \"경제 뉴스\"\n",
    "    \n",
    "    # 요청 헤더 설정 (브라우저처럼 요청 보내기 위해 User-Agent 포함)\n",
    "    req_header = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # 요청 보내기\n",
    "    res = requests.get(url, headers=req_header)\n",
    "    res.encoding = 'utf-8'  # 한글 깨짐 방지: 인코딩 설정은 soup 파싱 전에!\n",
    "\n",
    "    # 응답이 정상일 경우 HTML 파싱\n",
    "    if res.ok:\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # 뉴스 리스트 항목(li) 하나씩 순회\n",
    "        for li_tag in soup.select('ul.list_newsheadline2 li'):\n",
    "            a_tag = li_tag.find('a')         # 기사 링크 추출\n",
    "            if not a_tag: continue           # a 태그가 없으면 건너뜀\n",
    "            link = a_tag['href']             # 링크 주소 추출\n",
    "            print(link)\n",
    "\n",
    "            # 뉴스 제목 추출\n",
    "            strong_tags = li_tag.select('div.cont_thumb strong.tit_txt')\n",
    "            if strong_tags:\n",
    "                strong_tag = strong_tags[0]\n",
    "            title = strong_tag.text.strip()  # 텍스트만 추출 + 공백 제거\n",
    "            print(title)\n",
    "\n",
    "# 함수 호출 예시\n",
    "print_news('경제')  # https://news.daum.net/economy 경제 뉴스\n",
    "print_news('사회')  # https://news.daum.net/society 사회 뉴스\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
